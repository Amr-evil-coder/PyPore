{"name":"Pypore","tagline":"Tools used to analyze data from nanopore-based experiments.","body":"# PyPore\r\n## _Analysis of Nanopore Data_\r\n\r\nThe PyPore package is based off of a few core data analysis packages in order to provide a consistent and easy framework for handling nanopore data in the UCSC nanopore lab. The packages it requires are:\r\n\r\n* numpy\r\n* scipy\r\n* matplotlib\r\n* sklearn\r\n\r\nPackages which are not required, but can be used, are:\r\n\r\n* mySQLdb\r\n* cython\r\n* PyQt4\r\n\r\nLet's get started!\r\n\r\n# Files\r\n\r\nNanopore data files consist primarily of current levels corresponding to ions passing freely through the nanopore (\"open channel\"), and a blockages as something passes through the pore, such as a DNA strand (\"events\"). Data from nanopore experiments are stored in Axon Binary Files (extension .abf), as a sequence 32 bit floats, and supporting information about the hardware. They can be opened and loaded with the following:\r\n\r\n```\r\nfrom PyPore.DataTypes import *\r\nfile = File( \"My_File.abf\" ) \r\n```\r\n\r\nThe File class contains many methods to simplify the analysis of these files. The simplest analysis to do is to pull the events, or blockages of current, from the file, while ignoring open channel. Let's say that we are looking for any blockage of current which causes the current to dip from an open channel of ~120 pA. To be conservative, we set the threshold the current has to dip before being significant to 110 pA. This can be done simply with the file's parse method, which requires a parser class which will perform the parsing. The simplest event detector is the *lambda_event_parser, which has a keyword *threshold, indicating the raw current that serves as the threshold. \r\n\r\n```\r\nfrom PyPore.DataTypes import *\r\nfile = File( \"My_File.abf\" )\r\nfile.parse( parser=lambda_event_parser( threshold=110 ) ) \r\n```\r\n\r\nThe events are now stored as Event objects in file.events. The only other important file methods involve loading and saving them to a cache, which we'll cover later. Files also have the properties mean, std, and n (number of events). \r\n\r\n# Events\r\n\r\nEvents are segments of current which correspond to something passing through the nanopore. We hope that it is something which we are interested in, such as DNA or protein. An event is usually made up of a sequence of discrete segments of current, which should correspond to reading some region of whatever is passing through. In the best case, each discrete segment in an event corresponds to a single nucleotide of DNA, or a single amino acid of a protein passing through.\r\n\r\nEvents are often noisy, and transitions between them are quick, making filtering a good option for trying to see the underlying signal. Currently only [bessel filters](http://en.wikipedia.org/wiki/Bessel_filter) are supported for filtering tasks, as they've been shown to perform very well.\r\n\r\nLet's continue with our example, and imagine that now we want to filter each event, and look at it! The filter method has two parameters, order and cutoff, defaulting to order=1 and cutoff=2000. (Note that we now import pyplot as well.)\r\n\r\n```\r\nfrom PyPore.DataTypes import *\r\nfrom matplotlib import pyplot as plt\r\n\r\nfile = File( \"My_File.abf\" )\r\nfile.parse( parser=lambda_event_parser( threshold=110 ) ) \r\n\r\nfor event in file.events:\r\n    event.filter( order=1, cutoff=2000 )\r\n    event.plot()\r\n    plt.show()\r\n```\r\n\r\nCurrently, _lambda_event_parser( threshold )_ and _MemoryParse( starts, ends )_ are the only two parsers. MemoryParse takes in two lists, one of starts of events, and one of ends of events, and will cut a file into it's respective events. This is useful if you've done an analysis before and remember where the split points are. \r\n\r\nThe plot command will draw the event on whatever canvas you have, allowing you to make subplots with the events or add them into GUIs (such as Abada!), with the downside being that you need to use plt.show() after calling the plot command. The plot command wraps the pyplot.plot command, allowing you pass in any argument that could be used by pyplot.plot, for example:\r\n\r\n```\r\nevent.plot( alpha=0.5, marker='o' ) \r\n```\r\n\r\nThe next step is usually to try to segment this event into it's discrete states. There are several segmenters which have been written to do this, of which currently *StatSplit is the best, written by Dr. Kevin Karplus and based on a recursive maximum likelihood algorithm.  This algorithm was sped up by rewritting it in Cython, leading to *SpeedyStatSplit, which is a python wrapper for the cython code. Segmenting an event is the same process as detecting events in a file, by using the parse method on an event and passing in a parser.\r\n\r\nLet's say that now we want to segment an event and view it. Using the same plot command for the event, we can specify to color by 'cycle', which colors the segments in a four-color cycle for easy viewing. SpeedyStatSplit takes in several parameters, of which *min_gain_per_sample is the most important, and 0.50 to 1.50 usually provide an adequate level to parse at, with higher numbers leading to less segments. \r\n\r\n```\r\nfrom PyPore.DataTypes import *\r\nfrom matplotlib import pyplot as plt\r\n\r\nfile = File( \"My_File.abf\" )\r\nfile.parse( parser=lambda_event_parser( threshold=110 ) ) \r\n\r\nfor event in file.events:\r\n    event.filter( order=1, cutoff=2000 )\r\n    event.parse( parser=SpeedyStatSplit( min_gain_per_sample=0.50 ) )\r\n    event.plot( color='cycle' )\r\n    plt.show()\r\n```\r\n\r\nCurrently, the parsers implemented for segmentation are _SpeedyStatSplit( min_width, max_width, min_gain_per_sample, window_width, use_log, splitter)_, _StatSplit( *same as before* )_, _snakebase\\_parser( threshold, merger_thresh )_, and _novakker\\_parser( low_thresh, high_thresh, merger_thresh )_. SpeedyStatSplit is the cython implementation of the StatSplit code. Snakebase parser will segment whenever the distance between two adjacent local optima of the wave is above a threshold. The novakker parser will split whenever the derivative of the current is above a high threshold, and has passed below the low threshold since the last split. \r\n\r\nHowever, both Files and Events inherit from the Segment class, described below. This means that any of the parsers will work on either files or events, as long as they have an underlying .current attribute. \r\n\r\nThe last core functionality is the ability to apply an hidden markov model (HMM) to an event, and see which segments correspond to which hidden states. Any hmm (or more complex model!) can be used as long as it has a predict method (like sklearn hmms), but the PyPore.hmm module gives a core class, NanoporeHMM, and several examples, of how to make an hmm that will be useful. Let's say that we're dealing with an event that appears to switch between a high current state and a low current state, corresponding to a DNA strand ratcheting back and forth between two nucleotides. In order to try to group these segments, I've written an HMM named Bifurcator which will classify these segments as belonging to one group or the other. I want to visualize it's performance! \r\n\r\n```\r\nfrom PyPore.DataTypes import *\r\nfrom PyPore.hmm import Bifurcator\r\nfrom matplotlib import pyplot as plt\r\n\r\nfile = File( \"My_File.abf\" )\r\nfile.parse( parser=lambda_event_parser( threshold=110 ) ) \r\n\r\nfor event in file.events:\r\n    event.filter( order=1, cutoff=2000 )\r\n    event.parse( parser=SpeedyStatSplit( min_gain_per_sample=0.50 ) )\r\n    event.apply_hmm( hmm=Bifurcator )\r\n    event.plot( color='hmm' )\r\n    plt.show()\r\n```\r\n\r\nYou'll see that I had to import Bifurcator from the hmm module. By just using the event apply_hmm method, I can apply the hmm with minimal effort, and by coloring by 'hmm', segments will now be colored according to which hidden state in the HMM they correspond to. \r\n\r\nEvent objects also have the properties start, end, duration, mean, std, and n (number of segments after segmentation has been performed). \r\n\r\n# Segments\r\n\r\nSegments are short sequences of current samples, usually which appear to be from the same distribution. They are the core place where data are stored, as usually an event is analyzed by the metadata stored in each state. Segments have the attributes current, which stores the raw current samples, in addition to mean, std, duration, start, end, min, and max. They do not have any core methods.\r\n\r\nIf storing the raw sequence of current samples is too memory intensive, there are two ways to get rid of the current attribute. \r\n\r\n1) Initialize a MetaSegment object, instead of a Segment one, and feed in whatever statistics you'd like to save. This will prevent the current from ever being saved to a second object. For this example, lets assume you have a list of starts and ends of segments in an event, such as loading them from a cache.\r\n\r\n```\r\nevent = Event( current=[...], start=..., file=... )\r\nevent.segments =  [ MetaSegment( mean=np.mean( event.current[start:end] ),\r\n                                 std=np.std( event.current[start:end] ),\r\n                                 duration=(end-start)/100000 ) for start, end in zip( starts, ends ) ]\r\n```\r\n\r\nIn this example, references to the current are not stored in both the event and the segment, which may save memory if you wish to not store the raw current after analyzing a file. The duration here is divided by 100,000 because abf files store 100,000 samples per second, and we wished to convert from the integer index of the start and end to the second index of the start and end.\r\n\r\n2) If you have the memory to store the references, but don't want to accumulate them past a single event, you can parse a file normally, and produce normal segments, then call the function to_meta() to turn them into MetaSegments. This does not require any calculation on the user part, but does require the segment have contained all current samples at one point.\r\n\r\n```\r\nevent = Event( current=[...], start=..., file=... )\r\nevent.parse( parser=SpeedyStatSplit() )\r\nfor segment in event.segments:\r\n    segment.to_meta() \r\n```\r\n\r\n# Saving Analyses\r\n\r\nIf you perform an analysis and wish to save the results, there are multiple ways for you to do such. These operation seems common, for applications such as testing a HMM. If you write a HMM and want to make modifications to it, it would be useful to not have to redo the segmentation, but instead simply load up the same segmentation from the last time you did it. Alternatively, you may have a large batch of files you wish to analyze, and want to grab the metadata for each file to easily read after you go eat lunch, so you don't need to deal with the whole files.\r\n\r\n## MySQL Database:\r\nThe first is to store it to a MySQL database. The tables must be properly made for this-- see database.py if you want to see how to set up your own database to store PyPore results. If you are connected to the UCSC SoE secure network, there is a MySQL database, named chenoo, which will allow you to store an analysis. This is done on the file level, in order to preserve RDBMS format. \r\n\r\n```\r\nfrom PyPore.DataTypes import *\r\n\r\nfile = File( \"My_File.abf\" )\r\nfile.parse( parser=lambda_event_parser( threshold=110 ) ) \r\n\r\nfor event in file.events:\r\n    event.filter( order=1, cutoff=2000 )\r\n    event.parse( parser=SpeedyStatSplit( min_gain_per_sample=0.50 ) )\r\n    event.plot( color='cycle' )\r\n    plt.show()\r\nfile.to_database( database=\"chenoo\", host=\"...\", password=\"...\", user=\"...\" )\r\n```\r\n\r\nHost, password, and user must be set for your specific database. These files can then be read back by the following code.\r\n\r\n'''\r\nfrom PyPore.DataTypes import *\r\n\r\nfile = File.from_database( database=\"chenoo\", host=\"...\", password=\"...\", user=\"...\", AnalysisID, filename, eventDetector, eventDetectorParams, segmenter, segmenterParams, filterCutoff, filterOrder )\r\n\r\n'''\r\n\r\nThis will load the file back, with the previous segmentations. This will be anywhere from 10x to 1000x faster than performing the segmentation again. The time depends on how stable your connection with the database is, and how complex the analysis you did was. \r\n\r\nNow, it seems like there are a lot of parameters after user! You need to fill in as many of these as you can, to help identify which analysis you meant. AnalysisID is a primary key, but is also assigned by the database automatically when you stored it, so it is possible you do not know it. If you connect to MySQL independently and look up that ID, you can use it solely to identify which file you meant. If you do not provide enough information to uniquely identify a file, you may get an incorrect analysis.\r\n\r\n## JSON File\r\nA more portable and simple way to store analyses is to save the file to a json. This can be done simply with the following code.\r\n\r\n```\r\nfrom PyPore.DataTypes import *\r\n\r\nfile = File( \"My_File.abf\" )\r\nfile.parse( parser=lambda_event_parser( threshold=110 ) ) \r\n\r\nfor event in file.events:\r\n    event.filter( order=1, cutoff=2000 )\r\n    event.parse( parser=SpeedyStatSplit( min_gain_per_sample=0.50 ) )\r\n    event.plot( color='cycle' )\r\n    plt.show()\r\nfile.to_json( filename=\"My_File.json\" )\r\n```\r\n\r\nThe representation of your analysis will then be available as a human-readable json format. It may not be particularly fun to look at, but you will be able to read the metadata from the file. A snippet from an example file looks like the following:\r\n\r\n{\r\n     \"name\" : \"File\",\r\n     \"n\" : 16,\r\n     \"event_parser\" : {\r\n         \"threshold\" : 50.0,\r\n         \"name\" : \"lambda_event_parser\"\r\n     },\r\n     \"duration\" : 750.0,\r\n     \"filename\" : \"13823006-s06\",\r\n     \"events\" : [\r\n         {\r\n             \"std\" : 1.9335278997265508,\r\n             \"end\" : 31.26803,\r\n             \"state_parser\" : {\r\n                 \"min_gain_per_sample\" : 0.5,\r\n                 \"min_width\" : 1000,\r\n                 \"window_width\" : 10000,\r\n                 \"max_width\" : 1000000,\r\n                 \"name\" : \"SpeedyStatSplit\"\r\n             },\r\n             \"name\" : \"Event\",\r\n             \"min\" : 16.508111959669066,\r\n             \"max\" : 48.73997069621818,\r\n             \"segments\" : [\r\n                 {\r\n                     \"std\" : 2.8403093295527646,\r\n                     \"end\" : 0.01,\r\n                     \"name\" : \"Segment\",\r\n                     \"min\" : 22.330505378907066,\r\n                     \"max\" : 48.73997069621818,\r\n                     \"start\" : 0.0,\r\n                     \"duration\" : 0.01,\r\n                     \"mean\" : 27.341223956001969\r\n                 },\r\n                 {\r\n                     \"std\" : 0.5643329015609988,\r\n                     \"end\" : 2.5060499999999997,\r\n                     \"name\" : \"Segment\",\r\n                     \"min\" : 17.67660726490438,\r\n                     \"max\" : 26.554361458946911,\r\n                     \"start\" : 0.01,\r\n                     \"duration\" : 2.49605,\r\n                     \"mean\" : 24.084380592526145\r\n                ....\r\n\r\nThe file continues to list every event, and every segment in every event. The code to reconstruct an analysis from a json file is just as long as the code to reconstruct from the database.\r\n\r\n'''\r\nfrom PyPore.DataTypes import *\r\n\r\nfile = File.from_json( \"My_File.json\" )\r\n'''\r\n\r\nThis is usually faster than loading from a database, solely due to not having to connect across a network and stream data, and instead reading locally. ","google":"","note":"Don't delete this file! It's used internally to help with page regeneration."}